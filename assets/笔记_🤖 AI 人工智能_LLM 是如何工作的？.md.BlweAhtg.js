import{_ as u,c as h,I as r,j as e,w as n,a as t,au as f,D as o,o as p}from"./chunks/framework.RMxno62p.js";const d="/assets/how-llm-works-1.C_onc6jd.jpg",c="/assets/how-llm-works-aftermath-of-llm-video-1.CliJLhhh.mp4",b="/assets/how-llm-works-4.Cp-dLf6P.jpg",P="/assets/how-llm-works-2.IkdpnzT0.jpg",g="/assets/how-llm-works-agent-video-1.CA2G8F9n.mp4",A="/assets/how-llm-works-it-imagines-itself-video-1.BcwxwCc3.mp4",m="/assets/how-llm-works-3.B17amsDv.jpg",_="/assets/how-llm-works-5.BOOin7AD.png",we=JSON.parse('{"title":"LLM 是如何工作的？","description":"","frontmatter":{"progress":85},"headers":[],"relativePath":"笔记/🤖 AI 人工智能/LLM 是如何工作的？.md","filePath":"笔记/🤖 AI 人工智能/LLM 是如何工作的？.md"}'),H={name:"笔记/🤖 AI 人工智能/LLM 是如何工作的？.md"},k=e("h1",{id:"llm-是如何工作的",tabindex:"-1"},[t("LLM 是如何工作的？ "),e("a",{class:"header-anchor",href:"#llm-是如何工作的","aria-label":'Permalink to "LLM 是如何工作的？"'},"​")],-1),x=e("div",{class:"tip custom-block github-alert"},[e("p",{class:"custom-block-title"},"阅前须知"),e("p"),e("p",null,[t("先叠甲，这篇文档"),e("strong",null,"旨在给普罗大众"),t("推广和介绍 GPT 和 LLM 是什么？GPT 和 LLM 神奇在哪里？为什么会有 AI 热潮？怎么样能用好 GPT 和 LLM？与此同时，这篇文档假定大家（读者）"),e("strong",null,"只"),t("对 ChatGPT 和 LLM（大语言模型）有一个基础的认识。")]),e("p",null,"我知道这里面会有很多对于研究大语言模型的科研学者们早就已经滚瓜烂熟的知识，也会有很多对于提示词工程师和 GPT 大师们早就已经习得的最佳实践，有关和 ChatGPT 沟通的时候 do 和 don't do 的黄金守则，但是，为了能给普罗大众推广这些知识，我会通过参考很多现有的资料和资源，试图把很多复杂的概念简化，转写成猴子都能听懂的文字，避免不了地就会造成一部分的事实被过分简化，从而导致它看起来与实际的实现和情况不符。"),e("p",null,"如果你早就已经知道大语言模型的本质和如何使用它，可以跳到靠后的章节阅读，避免造成时间的二次浪费。"),e("p",null,"这是科普的时候时常有发生的事情，我会尽我所能解释清楚，并写附带上足够多的上下文说明这些过分简化的情况，以及补充足够多的解释和说明对那些感兴趣深入学习的读者深入阅读的资料和引用，对于无法周全满足，还请见谅，欢迎大家指正和提供更好的文档撰写的建议！")],-1),w=e("h2",{id:"洪水猛兽",tabindex:"-1"},[t("洪水猛兽 "),e("a",{class:"header-anchor",href:"#洪水猛兽","aria-label":'Permalink to "洪水猛兽"'},"​")],-1),z=e("blockquote",null,[e("p",null,"大型语言模型的现状")],-1),T=e("h3",{id:"毛孩子们",tabindex:"-1"},[t("毛孩子们 "),e("a",{class:"header-anchor",href:"#毛孩子们","aria-label":'Permalink to "毛孩子们"'},"​")],-1),L=e("blockquote",null,[e("p",null,"羊驼家族的冒险")],-1),y=e("video",{controls:"",muted:""},[e("source",{src:c,type:"video/mp4"})],-1),D=f('<h2 id="随机鹦鹉" tabindex="-1">随机鹦鹉 <a class="header-anchor" href="#随机鹦鹉" aria-label="Permalink to &quot;随机鹦鹉&quot;">​</a></h2><p>今天我想要从 ChatGPT 为什么会犯蠢和 ChatGPT 为什么总是满足不了我们开始说起，我想先聊一聊大家对于 ChatGPT 的误区。</p><p>我不知道是为什么，从何时开始的，包括去年的我在内，我和现在的绝大多数人对 ChatGPT 会有着非常高的预期，指望 ChatGPT 能回答很多对于 ChatGPT 而言不可能的和回答不好的问题。早在去年 OpenAI 刚发布 ChatGPT 之前我就参与到了内测里，并且在使用过 ChatGPT 之后我对 ChatGPT 所表现出来的、大家当时津津乐道的「<strong>搜索引擎</strong>」的能力感到非常的不屑，我经常无法得到很多我期望的回答。</p><p>对！就和我在这一章节所使用的标题「随机鹦鹉」一样，这些大语言模型看起来就是一群随机鹦鹉，这也是为什么在技术圈子里比较热门的 LLM 开发框架的图标会有「🦜」的 Emoji 蕴含在里面的原因。</p><p>是吧，那为什么呢？</p><p>我们先来看一下绝大多数人觉得 ChatGPT 不行的时候，都问了什么问题？是什么过高的预期造成的误解？</p><blockquote><p>今天是星期几？</p></blockquote><blockquote><p>张小明是谁？</p></blockquote><blockquote><p>你是 GPT4 吗？</p></blockquote><blockquote><p>今天股价是多少？</p></blockquote><blockquote><p>谁训练了你？</p></blockquote><blockquote><p>294712 x 12828 等于多少？</p></blockquote><p>这些问题都有一些共同的假设前提，那就是我们觉得</p><ol><li>这些事情足够简单，足够好问。</li><li>ChatGPT 怎么可能会不知道自己是谁呢？</li><li>既然 ChatGPT 叫做「人工智能」，那应该 100% 能像平时用计算机一样回答这些问题。</li><li>这些问题背后这都是在探讨既定事实，没有什么难计算和解答的。</li></ol><p>嗯，当然，确实挺简单的。让我们先稍微放一放，那更进阶的用户可能会提问说</p><blockquote><p>这看起来都是非常简单的问题，也都是事实强相关的问题，既然 AI 都是概率模型，那难道说别的问题它就能解决解决好了吗？明明很多时候它还是做不好事情</p></blockquote><p>当然，是的，会做不好事情！接下来我再来说一些稍为复杂一点点的、大家觉得 ChatGPT 能做、希望 ChatGPT 做，但是 ChatGPT 实际上，ChatGPT 和 LLM 真的做不到的事情。</p><ol><li>希望 ChatGPT 在解决自己认知范围内的时候超过自己。</li><li>希望 ChatGPT</li></ol><p>在这些情况下，ChatGPT 都会毫不吝啬地回答我们抛给它的问题，然后在它不知道这些问题的答案的时候胡编乱造一个答案出来，结果就是我们在和 ChatGPT 和 LLM 交互的过程中觉得：</p><blockquote><p>大语言模型经常会乱说，胡编乱造。</p></blockquote><p>对的，在这里要泼冷水的是，ChatGPT 无法在这样的预设前提下工作，我们也永远不应该以这样的方式去使用 ChatGPT 和类似的大语言模型。</p><blockquote><p>稍微卖点关子地说，我们也不应该用同样的预设去假定我们的人类同类能回答和解决这些问题。</p></blockquote><p>想要搞清楚为什么 ChatGPT 会再这样的问题下翻车，为什么 ChatGPT 总是表现得它自己很蠢，我们就得深入到 ChatGPT 的内部，了解一点点你可能早就听厌了的「Transformer」的原理，从 ChatGPT 和 LLM 的横切面上，剖析和理解 ChatGPT 是如何工作的，为什么它可以有很多人喜欢用的能力，探索一下对于绝大多数人而言，它又为什么如此的「不听话」和「不好用」，甚至胡编乱造的。</p><blockquote><p>提示词工程，上下文窗口，以及模型交互性的限制</p></blockquote><h3 id="活在幻觉里的-患有虚谈症的尸体" tabindex="-1">活在幻觉里的，患有虚谈症的尸体 <a class="header-anchor" href="#活在幻觉里的-患有虚谈症的尸体" aria-label="Permalink to &quot;活在幻觉里的，患有虚谈症的尸体&quot;">​</a></h3><p>不知道你是否有听说过「中文屋」？「中文屋」是这个世界上对人工智能，以及我们的意识，生命的本质讨论的最多的问题之一，时至今日我们都没办法找到解答的方法，</p>',26),j=f('<blockquote><p>语言模型的起源和基础</p></blockquote><p>这一切都要从 ChatGPT 和 LLM（大语言模型）所隶属的「自然语言处理（NLP）」的领域说起。</p><h3 id="「你所需要的一切」" tabindex="-1">「你所需要的一切」 <a class="header-anchor" href="#「你所需要的一切」" aria-label="Permalink to &quot;「你所需要的一切」&quot;">​</a></h3><p>想讨论这个部分就永远离不开 2017 年那篇开创性的论文「Attention is all you need（注意力就是你所需要的一切）」。</p><p>这篇论文提出并且实现了 Transformer 模型，并最终促成了 OpenAI 所研究的 GPT（Generative Pre-trained Transformer）和 Google 在研究的 BERT（Bidirectional Encoder Representation from Transformers）这类语言模型的诞生，使得它成为了许多「自然语言处理（NLP）」任务的首选模型，并且重新塑造了许多相关领域的工作流和研究方向。通过注意力机制，改善和抛弃了以前卷积神经网络和循环神经网络（RNN）的问题和效率。</p><p>那为什么，注意力就是我们所需要的一切？这一切和注意力有什么关系？</p><h4 id="从小到大-从大到小-大语言模型宇宙的基本粒子" tabindex="-1">从小到大，从大到小：大语言模型宇宙的基本粒子 <a class="header-anchor" href="#从小到大-从大到小-大语言模型宇宙的基本粒子" aria-label="Permalink to &quot;从小到大，从大到小：大语言模型宇宙的基本粒子&quot;">​</a></h4><p>One-hot 编码</p><p>Word Embedding 编码</p><p>BPE 编码</p>',10),C=f('<h4 id="像人一样思考" tabindex="-1">像人一样思考 <a class="header-anchor" href="#像人一样思考" aria-label="Permalink to &quot;像人一样思考&quot;">​</a></h4><p>你可能在许多科普视频或者论文解析中已经听过了 GPT 和 LLM 的底层原理，大家都会把 Transformer 架构下的 GPT 和 LLM 的行为看作是一种「单字接龙」游戏。</p><h5 id="什么是「单字接龙」" tabindex="-1">什么是「单字接龙」？ <a class="header-anchor" href="#什么是「单字接龙」" aria-label="Permalink to &quot;什么是「单字接龙」？&quot;">​</a></h5><blockquote><p>GPT 和 LLM 玩「单字接龙」游戏和这一切的能力又有什么关系呢</p></blockquote><p>我们不妨想象一下，我们作为人类，是如何理解一句话的。</p><p>接下来，我现在希望大家先和我一起做一个思想实验。</p><p>我想请问各位，当我们在</p><ul><li>写考试这样紧张的环境下书写自由答题类题目的答案的时候</li><li>和别人打电话的时候</li><li>在进行临时的即兴演讲的时候</li></ul><p>都是如何组织语言和造句的？</p><p>我不知道是不是每个人都是这样，我暂时还没有做过走访调查去研究大家的思维模式，但是就以我自己而言，在上面提及的这么多个场景中，我会有这样的思维模式：</p><ol><li>向前思考：有一部分思维在思考接下来要说的内容的结构</li><li>存储和追踪上下文：有一部分的记忆会维持和存储在对话过程中我们讨论的论点，论题，并且在过一段时间之后自己提醒我自己：「上下文在这里，别跑偏了」</li><li>拼接语言：我会根据自己平时听到过别人说话的方式，自己说话的习惯，逐渐拼凑和组织出完整的句子和逐渐在说话的过程中完善自己希望表达的想法和论点</li></ol><p>这个过程中最有意思的地方在于，在上述的场景下，我们似乎没有足够的时间拼凑和想清楚我们整个句子究竟是怎么说的，更没有足够的脑容量能够装得下说过的全部文字。</p><blockquote><p>我似乎是在不断说出每个字的过程中，不断更新自己的想法，以此修正自己的用语和用词，最终更新自己想说的话。</p></blockquote><p>更有意思的是，在 Transformer 架构下，GPT 和 LLM 也是在以这样的行为、结构和逻辑中思考的。</p><p>那 Transformer 模型里是如何运作的呢？</p><blockquote><p>TODO</p></blockquote><p>那问题来了，「单字接龙」难在哪里？</p><details class="details custom-block"><summary>💡 额外的思考：这与「完形填空」有什么区别？「完形填空」也可以实现和达到一样的大语言模型的水平吗？</summary><p>有意思的是，Google 一直在研究的 BERT（Bidirectional Encoder Representation from Transformers）其实就是「完形填空」形式的训练逻辑。</p></details><blockquote><p>注意力机制是一种允许模型在处理输入数据时，关注在不同位置的不同信息的技术。在语言模型中，这意味着模型可以关注输入句子中的不同部分，而不仅仅是当前处理的单词。 这种机制使 Transformer 能够更好地理解语言中的上下文关系，尤其是长距离的依赖关系。</p></blockquote><table tabindex="0"><thead><tr><th>名称</th><th>含义</th></tr></thead><tbody><tr><td>Q</td><td>Query</td></tr><tr><td>K</td><td>Key</td></tr><tr><td>V</td><td>Value</td></tr></tbody></table><p>这是不是过于简单了？说了等于没说。</p><blockquote><p>TODO</p></blockquote><p>其实从本质上来说，Q，K，V 三个向量，他们各自的行为模式，就真的像是我们平时在与数据库，或者代码中的字典交互的时候所做的 Query（查询），Key（键）以及 Value（值）那样工作，只不过是以向量为内容的形式来构建的。</p><blockquote><p>TODO</p></blockquote><h4 id="流动的神经元" tabindex="-1">流动的神经元 <a class="header-anchor" href="#流动的神经元" aria-label="Permalink to &quot;流动的神经元&quot;">​</a></h4><p>Transformer 通过使用自注意力（self-attention）机制和位置编码（positional encoding），可以处理文本中的长距离依赖问题，并保持高效的并行计算。</p><blockquote><p>TODO</p></blockquote><p>前馈神经网络。</p><blockquote><p>TODO</p></blockquote><p>反向传播。</p><blockquote><p>TODO</p></blockquote><h4 id="拟合-一切都可以拟合" tabindex="-1">拟合，一切都可以拟合 <a class="header-anchor" href="#拟合-一切都可以拟合" aria-label="Permalink to &quot;拟合，一切都可以拟合&quot;">​</a></h4><p>我不知道大家有没有在过去的 GPT（生成式预训练 Transformer）和 AI（人工智能）发展的这段时间里回忆起和想过，为什么在 2020 年和 2021 年的时候，大家普遍在做的 GPT 和 LLM 应用不是现在 ChatGPT 这样的问答式 Bot，而是文本和小说续写，以及 GitHub Copilot 那样的代码生成模型呢？</p><h3 id="真的很大" tabindex="-1">真的很大 <a class="header-anchor" href="#真的很大" aria-label="Permalink to &quot;真的很大&quot;">​</a></h3>',34),q=e("h3",{id:"但它也没有那么大",tabindex:"-1"},[t("但它也没有那么大 "),e("a",{class:"header-anchor",href:"#但它也没有那么大","aria-label":'Permalink to "但它也没有那么大"'},"​")],-1),O=e("p",null,"其实自注意力机制在「Attention is all you need（注意力就是你所需要的一切）」论文诞生之前就被很多研究员以及科研学者提及过，但是他们都因为？",-1),G=e("p",null,"RNN，GRU，LASTM 窗口不足。",-1),M=e("blockquote",null,[e("p",null,"Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall We all love increasing context lengths - but what's performance like? Anthropic reached out with early access to Claude 2.1 so I repeated the “needle in a haystack” analysis I did on GPT-4 Here's what I found..."),e("p",null,'Greg Kamradt 对 GPT-4 (128K) 与 Claude 2.1 (200K) 进行了名为"大海捞针"的长上下文精度测试。实验了两个AI在接收不同长度的上下文时，对文档中不同位置的内容，有何记忆上的差异。'),e("p",null,[e("strong",null,"测试结果 :")]),e("ul",null,[e("li",null,"AI 更容易记住（无论长度）: 文本后半部分。"),e("li",null,"AI 更不容易记住（90K 长文时）: 文本前半部分。"),e("li",null,"AI 近乎 100% 记住（无论长度) : 文本开头 & 文本结尾。"),e("li",null,"越少的上下文 = 越高的准确性。"),e("li",null,"测试的 API 调用成本约为 1016 美元。")])],-1),I=e("h2",{id:"原初智能",tabindex:"-1"},[t("原初智能 "),e("a",{class:"header-anchor",href:"#原初智能","aria-label":'Permalink to "原初智能"'},"​")],-1),N=e("blockquote",null,[e("p",null,"利用Agent和工具增强模型的泛化能力")],-1),X=e("p",null,"开源中文指令通用语料库",-1),B=e("p",null,"左脚踩右脚就可以上天",-1),E=e("h3",{id:"用乐高的方式构建和延展智能",tabindex:"-1"},[t("用乐高的方式构建和延展智能 "),e("a",{class:"header-anchor",href:"#用乐高的方式构建和延展智能","aria-label":'Permalink to "用乐高的方式构建和延展智能"'},"​")],-1),S=e("p",null,"斯坦福的人机交互小组用大语言模型做了一个有二十五个自由自在生活的 AI 的小镇。",-1),V=e("p",null,"在评估中，这些生成代理产生可信度高且涌现性的社会行为：例如仅从单个用户指定一个想要举办情人节派对的概念开始，该派对自主地传播邀请两天后结识新朋友，互相邀请参加派对，并协调在正确的时间一起出现。",-1),R=e("p",null,"我们通过消融实验表明，代理架构的组成部分——观察、规划和反思——每个都对代理行为的可信度做出了重要贡献。",-1),K=e("p",null,"通过将大型语言模型与计算交互代理相融合，这项工作引入了架构和交互模式，以实现对人类行为的可信模拟。",-1),Y=e("h3",{id:"积木的魔力",tabindex:"-1"},[t("积木的魔力 "),e("a",{class:"header-anchor",href:"#积木的魔力","aria-label":'Permalink to "积木的魔力"'},"​")],-1),Q=e("li",null,"ChatGPT 插件",-1),W=e("li",null,"操作 Microsoft Office 全家桶",-1),J=e("li",null,"操作 Notion 中的知识，把 Notion 作为知识库",-1),Z=e("p",null,"眼睛，耳朵，四肢，都可以是 Agent",-1),F=e("video",{controls:"",muted:""},[e("source",{src:g,type:"video/mp4"})],-1),U=e("blockquote",null,[e("p",null,"TidyBot: Personalized Robot Assistance with Large Language Models approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts…")],-1),$=e("blockquote",null,[e("p",null,"论文作者提出宏大的 TaskMatrix AI 平台，利用 LLM 集成已有的 API，在数字和物理领域实现多样化的任务。这篇论文出自微软员工，阅读中感觉像是在看 ChatGPT Plugin 的工程实现。")],-1),ee=e("p",null,"对，多模态也可以是 Agent",-1),te=e("p",null,"甚至可以让它想象它自己的模样，然后用 Diffusion 模型画出来",-1),re=e("video",{controls:"",muted:""},[e("source",{src:A,type:"video/mp4"})],-1),ae=e("blockquote",null,[e("p",null,'This is how GPT-4 sees and hears itself" I used GPT-4 to describe itself. Then I used its description to generate an image, a video based on this image and a soundtrack. Tools I used: GPT-4, Midjourney, Kainber AI, Mubert, RunwayML This is the description I used that GPT-4...')],-1),ne=e("h3",{id:"langchain-和-llamaindex-都做了什么",tabindex:"-1"},[t("LangChain 和 LlamaIndex 都做了什么？ "),e("a",{class:"header-anchor",href:"#langchain-和-llamaindex-都做了什么","aria-label":'Permalink to "LangChain 和 LlamaIndex 都做了什么？"'},"​")],-1),le=e("h2",{id:"我们并无二致",tabindex:"-1"},[t("我们并无二致 "),e("a",{class:"header-anchor",href:"#我们并无二致","aria-label":'Permalink to "我们并无二致"'},"​")],-1),oe=e("h3",{id:"prompt-injection",tabindex:"-1"},[t("Prompt Injection "),e("a",{class:"header-anchor",href:"#prompt-injection","aria-label":'Permalink to "Prompt Injection"'},"​")],-1),fe=e("blockquote",null,[e("p",null,"论文研究了5个最先进的语言模型 (ChatGPT 系列、Claude 系列、LLaMA 2)，确认这些基于人类反馈强化学习 (RLHF) 的 AI 普遍会对人类阿谀奉承。当人类有先入为主的观点时它会主动贴合，当被质疑时它会认错，甚至将正确答案修改为错误答案。"),e("p",null,"Anthropic 发现可能是 RLHF 教育出了这种“马屁精”，这种学习方式虽然在生产高质量 AI 方面具有明显效用，但通过贴合人类偏好激励的 AI 会牺牲自己的真实性来“谄媚”人类，人们需要改进训练方法。")],-1),ve=f('<h3 id="adversarial-prompt-attack" tabindex="-1">Adversarial Prompt Attack <a class="header-anchor" href="#adversarial-prompt-attack" aria-label="Permalink to &quot;Adversarial Prompt Attack&quot;">​</a></h3><ol><li>输入预处理：直接检测和处理可能的对抗样本，如检测错别字、无关的序列，并提高提示的清晰度和简洁度。</li><li>在预训练中包含低质量数据：低质量数据可以作为可能的对抗样本，在预训练中包含低质量数据可能会对多样化的输入有更好的理解。</li><li>探索改进的微调方法：研究更佳的微调技术可能会提高鲁棒性。正如我们之前展示的，比如T5和UL2模型比ChatGPT的鲁棒性更好，这暗示了大规模监督微调的潜在优势。</li></ol><h2 id="now-what" tabindex="-1">Now What? <a class="header-anchor" href="#now-what" aria-label="Permalink to &quot;Now What?&quot;">​</a></h2><p>作为普通人，如何接触和使用到大语言模型？</p><blockquote><p>大型语言模型带来的影响和未来的展望</p></blockquote>',5),ie=e("p",null,"Poe 发布面向开发者的 API",-1),se=f('<p>AI 涉及到方方面面，它不是一个独立的领域，但是它也有自己的基础设施，工作流。</p><p>对于大语言模型尚未如此热门之前，大家的工作流是这样的</p><p>数据标注，数据湖仓，数据提炼</p><p>模型训练，模型微调</p><p>模型部署</p><p>在大语言模型时代，新的领域被催生了</p><p>帮助提炼文档特征的文档工程师</p><p>提示词工程</p><p>模型小型化</p><p>Agent</p><p>Agent 和提示词编排</p><p>大语言模型审计和数据安全</p><p>提示词注入</p><h2 id="临时" tabindex="-1">临时 <a class="header-anchor" href="#临时" aria-label="Permalink to &quot;临时&quot;">​</a></h2><p>我关注的过去一周的 AI 热门项目：</p><p>XAgent 开源，比 AutoGPT 更加稳定和精确的复杂任务调度、设计、执行和落地的全自动 GPT Agent，与 Langchain 这样的单 Agent 或者多个单 Agent 智能体执行任务，以及 AutoGPT 执行任务的时候容易陷入自己的死循环和对错误进行错误处理和产生执念不同，XAgent 会将任务拆解和规划，逐步使用自己创建和调度的子 Agent 进行任务处理，甚至实现自己为了解决某个问题而单独训练一个小模型的情况，与 AutoGPT 会死循环不同，XAgent 添加了能找人类或者多个数据 ETL 模块进行求助的行为模式来允许自动任务发生中断</p><p>OpenBMB/XAgent: An Autonomous LLM Agent for Complex Task Solving</p>',17),ue=e("p",null,"Prompt flow 开源，支持在 vscode 中流式可视化编辑和开发 GPT Agent，方便为 LLM 应用解决原型构建，基准测试，以及生产落地和监控。",-1),he=e("p",null,"microsoft/promptflow: Build high-quality LLM apps - from prototyping, testing to production deployment and monitoring.",-1),pe=e("p",null,"看 NVIDIA 发了研究 Blog 说自己用类似于 XAgent 外循环 + 内循环的方式去让小模型和数字孪生能够对「对用人手进行转笔这样的动作进行建模」这样的复杂任务进行微调和监督，实现更全面和智能的无监督学习。",-1),de=e("p",null,"在 Minecraft 中玩游戏",-1),ce=e("h2",{id:"参考资料",tabindex:"-1"},[t("参考资料 "),e("a",{class:"header-anchor",href:"#参考资料","aria-label":'Permalink to "参考资料"'},"​")],-1),be=e("h2",{id:"延伸阅读",tabindex:"-1"},[t("延伸阅读 "),e("a",{class:"header-anchor",href:"#延伸阅读","aria-label":'Permalink to "延伸阅读"'},"​")],-1);function Pe(ge,Ae,me,_e,He,ke){const v=o("NolebasePageProperties"),l=o("NolebaseUnlazyImg"),a=o("VPNolebaseInlineLinkPreview"),i=o("NolebaseGitContributors"),s=o("NolebaseGitChangelog");return p(),h("div",null,[k,r(v),x,e("p",null,[r(l,{src:d,alt:"",thumbhash:"OQgGBwD423RqepepV0eWipW3C7mJBHcE",placeholderSrc:"data:image/png;base64,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",width:"1280",height:"1280",autoSizes:"true"})]),w,z,e("p",null,[r(a,{href:"https://chaudhry.notion.site/I-wish-GPT4-had-never-happened-9f0cbf2848a44ec9911c07fb34ff5de3",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("I wish GPT4 had never happened")]),_:1})]),T,L,y,e("p",null,[r(a,{href:"https://github.com/BlinkDL/ChatRWKV",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("BlinkDL/ChatRWKV: ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.")]),_:1})]),e("p",null,[r(a,{href:"https://github.com/Vision-CAIR/MiniGPT-4",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Vision-CAIR/MiniGPT-4: Open-sourced codes for MiniGPT-4 and MiniGPT-v2")]),_:1}),t(" ("),r(a,{href:"https://minigpt-4.github.io",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://minigpt-4.github.io")]),_:1}),t(", "),r(a,{href:"https://minigpt-v2.github.io/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://minigpt-v2.github.io/")]),_:1}),t(")")]),e("p",null,[r(a,{href:"https://twitter.com/nash_su/status/1651450879122501632",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://twitter.com/nash_su/status/1651450879122501632")]),_:1})]),e("p",null,[r(a,{href:"https://twitter.com/bananadev_/status/1648862816294834177",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("StableDiffustion 的缔造者 Stability AI 发布 StableLM")]),_:1})]),e("p",null,[r(a,{href:"https://zhuanlan.zhihu.com/p/628650749",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("PaLM 2 Technical Report 速读简报 - 知乎")]),_:1})]),e("p",null,[r(a,{href:"https://huggingface.co/bigcode",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("bigcode (BigCode)")]),_:1})]),e("p",null,[r(a,{href:"https://www.inmediahk.net/node/%E6%95%99%E8%82%B2/%E6%B8%AF%E5%A4%A7%E8%A7%A3%E7%A6%81chatgpt-9%E6%9C%88%E8%B5%B7%E5%85%8D%E8%B2%BB%E7%94%A8-%E5%AD%B8%E7%94%9F%E6%AF%8F%E6%9C%88%E9%99%90%E7%99%BC20%E6%A2%9D%E6%8C%87%E4%BB%A4",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("港大解禁ChatGPT 9月起免費用 學生每月限發20條指令 | 獨媒報導 | 獨立媒體")]),_:1})]),D,e("p",null,[r(a,{href:"https://www.beren.io/2023-03-19-LLMs-confabulate-not-hallucinate/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("LLMs confabulate not hallucinate")]),_:1})]),j,e("p",null,[r(a,{href:"https://zh.wikipedia.org/zh-cn/%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("字节对编码 - 维基百科，自由的百科全书")]),_:1}),r(a,{href:"https://zhuanlan.zhihu.com/p/383650769",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("一文搞懂BPE分词算法 - 知乎")]),_:1}),r(a,{href:"https://www.less-bug.com/posts/using-bpe-principle-for-chinese-word-segmentation-plate/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("使用 BPE 原理进行汉语字词切分（重制版）")]),_:1})]),e("p",null,[t("Two minutes NLP — A Taxonomy of Tokenization Methods | by Fabio Chiusano | NLPlanet | Medium "),r(a,{href:"https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://medium.com/nlplanet/two-minutes-nlp-a-taxonomy-of-tokenization-methods-60e330aacad3")]),_:1})]),C,e("p",null,[r(a,{href:"https://youtu.be/-HYbFm67Gs8",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://youtu.be/-HYbFm67Gs8")]),_:1})]),q,O,G,e("p",null,[r(a,{href:"https://arxiv.org/pdf/2304.11062.pdf",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("arXiv [2304.11062] Scaling Transformer to 1M tokens and beyond with RMT")]),_:1})]),e("p",null,[r(l,{src:b,alt:"",thumbhash:"8OcNDIjnuIeFiIiPdnmgawe7pg==",placeholderSrc:"data:image/png;base64,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",width:"1280",height:"714",autoSizes:"true"})]),M,e("p",null,[r(a,{href:"https://twitter.com/GregKamradt/status/1727018183608193393",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://twitter.com/GregKamradt/status/1727018183608193393")]),_:1})]),e("p",null,[r(a,{href:"https://x.com/dotey/status/1727437625194136060",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://x.com/dotey/status/1727437625194136060")]),_:1})]),e("p",null,[r(a,{href:"https://x.com/dotey/status/1727454708627808261",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://x.com/dotey/status/1727454708627808261")]),_:1})]),I,N,e("p",null,[r(a,{href:"https://orangeblog.notion.site/GPT-4-8fc50010291d47efb92cbbd668c8c893",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("《GPT-4 ，通用人工智能的火花》论文内容精选与翻译")]),_:1})]),e("p",null,[r(a,{href:"https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756#e5422f6579d8440f9f592eb03e28eb38",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("拆解追溯 GPT-3.5 各项能力的起源")]),_:1})]),e("p",null,[r(a,{href:"https://t.co/LPvuuxysCr",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("arXiv [2305.03047 ] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision")]),_:1})]),e("p",null,[r(a,{href:"https://github.com/IBM/Dromedary",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("IBM/Dromedary: Dromedary: towards helpful, ethical and reliable LLMs.")]),_:1})]),X,e("p",null,[r(a,{href:"https://arxiv.org/abs/2304.07987",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("arXiv [2304.07987] Chinese Open Instruction Generalist: A Preliminary Release")]),_:1})]),B,e("p",null,[r(a,{href:"https://github.com/project-baize/baize-chatbot",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("project-baize/baize-chatbot: Let ChatGPT teach your own chatbot in hours with a single GPU!")]),_:1})]),E,S,e("p",null,[r(l,{src:P,alt:"",thumbhash:"6dkFFIrUkonAdtp8aIj0oY6RCQ==",placeholderSrc:"data:image/png;base64,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",width:"1280",height:"658",autoSizes:"true"})]),V,R,K,e("p",null,[r(a,{href:"https://reverie.herokuapp.com/arXiv_Demo/#",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://reverie.herokuapp.com/arXiv_Demo/#")]),_:1})]),e("p",null,[r(a,{href:"https://arxiv.org/abs/2304.03442",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://arxiv.org/abs/2304.03442")]),_:1})]),e("p",null,[r(a,{href:"https://www.yystv.cn/p/10710",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("由 25 个 AI 智能体组成的虚拟小镇，会产生自由意志吗？ - 游研社")]),_:1})]),e("p",null,[r(a,{href:"https://lilianweng.github.io/posts/2023-06-23-agent/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("LLM Powered Autonomous Agents | Lil'Log")]),_:1})]),e("p",null,[r(a,{href:"https://mp.weixin.qq.com/s/bV1tPc7hNn2z06YOpzyanw",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("AutoGPT太火了，无需人类插手自主完成任务，GitHub2.7万星")]),_:1})]),e("p",null,[r(a,{href:"https://three-recorder-52a.notion.site/Agent-7b4bc7a71f8d4d4b940abc9b3232954a",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Auto Agent 相关的文章合集")]),_:1})]),Y,e("ul",null,[e("li",null,[r(a,{href:"https://www.phind.com/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("面向开发者的搜索引擎")]),_:1})]),Q,e("li",null,[r(a,{href:"https://twitter.com/benyu0620/status/1651498026085785601",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("操作 Android")]),_:1})]),W,J,e("li",null,[r(a,{href:"https://finchat.io/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("金融好帮手")]),_:1})]),e("li",null,[r(a,{href:"https://twitter.com/mattshumer_/status/1655954393823363072",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("产品经理")]),_:1})]),e("li",null,[r(a,{href:"https://www.atlassian.com/blog/announcements/unleashing-power-of-ai",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Confluence 和 Jira 也可以有 AI 助理助力")]),_:1})]),e("li",null,[r(a,{href:"https://twitter.com/DrJimFan/status/1662115266933972993",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("玩 Minecraft")]),_:1})]),e("li",null,[r(a,{href:"https://mp.weixin.qq.com/s?__biz=MzkyNTI4NzI2OQ==&mid=2247484080&idx=1&sn=7155d4aeb8a8eadf25a86972eee04119",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("如何为 chatGPT 增加网络访问功能")]),_:1})])]),Z,F,U,e("p",null,[r(a,{href:"https://twitter.com/_akhaliq/status/1656117478760796160",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://twitter.com/_akhaliq/status/1656117478760796160")]),_:1})]),$,e("p",null,[r(a,{href:"https://briefgpt.xyz/a/2303.16434",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("TaskMatrix.AI：通过连接基础模型和数百万个 API 完成任务 | BriefGPT - AI 论文速递")]),_:1})]),ee,e("p",null,[r(a,{href:"https://lilianweng.github.io/posts/2022-06-09-vlm/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Generalized Visual Language Models | Lil'Log")]),_:1})]),e("p",null,[r(a,{href:"https://blog.langchain.dev/agents-round/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Autonomous Agents & Agent Simulations")]),_:1})]),te,re,ae,e("p",null,[r(a,{href:"https://twitter.com/icreatelife/status/1649873812295491584",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://twitter.com/icreatelife/status/1649873812295491584")]),_:1})]),e("p",null,[r(a,{href:"https://www.coze.com/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("字节跳动出品的可以调用 GPT4 的 GPTs 平台 - Coze")]),_:1})]),ne,e("p",null,[r(a,{href:"https://mp.weixin.qq.com/s/3coFhAdzr40tozn8f9Dc-w",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("LangChain：Model as a Service粘合剂，被ChatGPT插件干掉了吗？")]),_:1})]),le,oe,e("p",null,[r(a,{href:"https://arxiv.org/abs/2308.09687",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("arXiv [2308.09687] Graph of Thoughts: Solving Elaborate Problems with Large Language Models")]),_:1})]),e("p",null,[r(l,{src:m,alt:"",thumbhash:"OAgGC4J49XmWV8h5j1TiX0Y=",placeholderSrc:"data:image/png;base64,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",width:"1280",height:"459",autoSizes:"true"})]),fe,e("p",null,[r(a,{href:"https://arxiv.org/abs/2310.13548",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("arXiv [2310.13548] Towards Understanding Sycophancy in Language Models")]),_:1})]),e("p",null,[r(a,{href:"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Adversarial Attacks on LLMs | Lil'Log")]),_:1})]),e("p",null,[r(a,{href:"https://simonwillison.net/2023/Apr/14/worst-that-can-happen/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Prompt injection: What’s the worst that can happen?")]),_:1})]),e("p",null,[r(a,{href:"https://mp.weixin.qq.com/s?__biz=Mzg3MjY5Mzc5Mg==&mid=2247483699&idx=1&sn=98dde197f941dcddc0c90ee6881cf1e8",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Notion AI'Prompt的逆向| Reverse Prompt Engineering for Fun(译文)")]),_:1})]),e("p",null,[r(a,{href:"https://mp.weixin.qq.com/s?__biz=Mzg3MjY5Mzc5Mg==&mid=2247483793&idx=1&sn=4456c7805964af58356b03cb75bb6432",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("LLM 中的安全隐患 - 提示注入 Prompt injection")]),_:1})]),e("p",null,[r(a,{href:"https://github.com/microsoft/promptbench",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("microsoft/promptbench: A unified evaluation framework for large language models")]),_:1})]),e("p",null,[r(a,{href:"https://www.bilibili.com/video/BV17X4y1W74A",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("大模型对Prompt的鲁棒性评估基准: PromptBench （大模型时代的科研之3）- 哔哩哔哩 bilibili")]),_:1})]),e("p",null,[r(a,{href:"https://zhuanlan.zhihu.com/p/637219127",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("大模型鲁棒不鲁棒，PromptBench测一测: 首个大语言模型提示鲁棒性的评测基准PromptBench - 知乎")]),_:1})]),e("p",null,[t("ChatGPT 的 System Prompt "),r(a,{href:"https://github.com/LouisShark/chatgpt_system_prompt",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("LouisShark/chatgpt_system_prompt: collect agent's system prompt and share some prompt inject knowledge")]),_:1})]),ve,e("p",null,[r(a,{href:"https://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("The architecture of today's LLM applications - The GitHub Blog")]),_:1})]),e("p",null,[r(a,{href:"https://huyenchip.com/2023/04/11/llm-engineering.html",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Building LLM applications for production")]),_:1})]),e("p",null,[r(a,{href:"https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("ColossalAI/docs/README-zh-Hans.md at main · hpcaitech/ColossalAI")]),_:1})]),e("p",null,[r(l,{src:_,alt:"",thumbhash:"+/cBA4CSV7CYeVl3qkx/jPQ=",placeholderSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAOCAYAAABO3B6yAAAHWklEQVR4AQCBAH7/AOvv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/AIEAfv8A6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f8AgQB+/wDr7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/wCBAH7/AOvv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/AIEAfv8A6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f8AgQB+/wDr7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/wCBAH7/AOvv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/AIEAfv8A6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f8AgQB+/wDr7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/wCBAH7/AOvv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/AIEAfv8A6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f8AgQB+/wDr7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/wCBAH7/AOvv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/AYEAfv8A6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/r7/H/6+/x/+vv8f/aKqHbLHrn+gAAAABJRU5ErkJggg==",width:"3280",height:"1514",autoSizes:"true"})]),e("blockquote",null,[e("p",null,[t("Hongyi Jin：“Introducing WebLLM, an open-source chatbot that brings language models (LLMs) directly onto web browsers. We can now run instruction fine-tuned LLaMA (Vicuna) models natively on your browser tab via @WebGPU with no server support. Checkout our demo at "),r(a,{href:"https://t.co/dXII0MzYg1",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://t.co/dXII0MzYg1")]),_:1}),t(" . "),r(a,{href:"https://t.co/IfgwPq0GTE%E2%80%9D",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://t.co/IfgwPq0GTE”")]),_:1}),t(" / X")])]),e("p",null,[r(a,{href:"https://twitter.com/hongyijin258/status/1647062309960028160",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://twitter.com/hongyijin258/status/1647062309960028160")]),_:1})]),e("p",null,[r(a,{href:"https://sourcegraph.com/blog/cody-is-generally-available",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("SourceGraph 宣布 Cody GA")]),_:1})]),ie,e("p",null,[r(a,{href:"https://twitter.com/adamdangelo/status/1658121701077516291",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://twitter.com/adamdangelo/status/1658121701077516291")]),_:1})]),e("p",null,[r(a,{href:"https://mp.weixin.qq.com/s/HhIGAojnZVSu4vMBpMP4yA",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("DeepSpeed Chat：一键搞定不同规模 ChatGPT 类模型训练！")]),_:1})]),e("p",null,[r(a,{href:"https://hacks.mozilla.org/2023/11/introducing-llamafile/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Introducing llamafile - Mozilla Hacks - the Web developer blog")]),_:1})]),e("p",null,[r(a,{href:"https://github.com/Mozilla-Ocho/llamafile?utm_source=substack&utm_medium=email",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Mozilla-Ocho/llamafile: Distribute and run LLMs with a single file.")]),_:1})]),se,e("p",null,[r(a,{href:"https://github.com/OpenBMB/XAgent",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://github.com/OpenBMB/XAgent")]),_:1})]),ue,he,e("p",null,[r(a,{href:"https://github.com/microsoft/promptflow",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://github.com/microsoft/promptflow")]),_:1})]),e("p",null,[t("AutoGen 开源，是 "),r(a,{href:"https://github.com/microsoft/FLAML%EF%BC%88",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://github.com/microsoft/FLAML（")]),_:1}),t(" 自动机器学习和全自动微调框架 ）的衍生品，相比 AutoGPT 而言，这个项目旨在提供更多的多 agent 协作的工具，可以理解为 langchain multi agent 的平替，也可以理解为可以用 AutoGen 可以配合 Prompt flow 拼出一个 XAgent")]),e("p",null,[t("microsoft/autogen: Enable Next-Gen Large Language Model Applications. Join our Discord: "),r(a,{href:"https://discord.gg/pAbnFJrkgZ",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://discord.gg/pAbnFJrkgZ")]),_:1})]),e("p",null,[r(a,{href:"https://github.com/microsoft/autogen",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://github.com/microsoft/autogen")]),_:1})]),pe,e("p",null,[r(a,{href:"https://blogs.nvidia.com/blog/2023/10/20/eureka-robotics-research/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://blogs.nvidia.com/blog/2023/10/20/eureka-robotics-research/")]),_:1})]),de,e("p",null,[t("MineDojo/Voyager: An Open-Ended Embodied Agent with Large Language Models "),r(a,{href:"https://github.com/MineDojo/Voyager",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://github.com/MineDojo/Voyager")]),_:1})]),e("p",null,[r(a,{href:"https://arxiv.org/abs/2305.16291",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("arXiv [2305.16291] Voyager: An Open-Ended Embodied Agent with Large Language Models")]),_:1})]),e("p",null,[t("Voyager | An Open-Ended Embodied Agent with Large Language Models "),r(a,{href:"https://voyager.minedojo.org/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://voyager.minedojo.org/")]),_:1})]),e("p",null,[r(a,{href:"https://www.cnbeta.com.tw/articles/tech/1396051.htm",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("新型人工智能算法可在5秒钟内从2D图像中创建3D模型 - VR / AR / 3D / IMAX - cnBeta.COM")]),_:1})]),e("p",null,[t("Frameworks for Serving LLMs. A comprehensive guide into LLMs inference and serving | by Sergei Savvov | Jul, 2023 | Medium | Better Programming "),r(a,{href:"https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407")]),_:1})]),e("blockquote",null,[e("p",null,[t('X 上的 fin：“这是一篇打破GPT“涌现”概念神话的论文，终于说出了我一直以来的一个直觉，这才是比较符合事物发展规律的 一句话总结，所谓GPT“涌现”能力，是因为人为修改了“达标”的评价标准，给人"涌现"的错觉 一旦使用更合理的评价指标，就会发现GPT能力值随着模型增大是线性增长的，从评价指标上直接解构了“涌现”… '),r(a,{href:"https://t.co/NJv7jCjM4h%E2%80%9D",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://t.co/NJv7jCjM4h”")]),_:1}),t(" / X "),r(a,{href:"https://twitter.com/fi56622380/status/1654386086746132481",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://twitter.com/fi56622380/status/1654386086746132481")]),_:1})])]),e("p",null,[t("Nature：DeepMind大模型突破60年数学难题 解法超出人类已有认知 - AI 人工智能 - cnBeta.COM "),r(a,{href:"https://www.cnbeta.com.tw/articles/tech/1404741.htm",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("https://www.cnbeta.com.tw/articles/tech/1404741.htm")]),_:1})]),ce,e("p",null,[r(a,{href:"https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("LLM Introduction: Learn Language Models")]),_:1})]),e("p",null,[r(a,{href:"https://typefully.com/DanHollick/how-chatgpt-works-a-deep-dive-yA3ppZC",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("How ChatGPT works: a deep dive | Dan Hollick")]),_:1})]),e("p",null,[r(a,{href:"https://www.ted.com/talks/greg_brockman_the_inside_story_of_chatgpt_s_astonishing_potential",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Greg Brockman: The inside story of ChatGPT's astonishing potential | TED Talk")]),_:1})]),e("p",null,[r(a,{href:"https://txt.cohere.com/what-are-transformer-models/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("What Are Transformer Models and How Do They Work?")]),_:1})]),e("p",null,[r(a,{href:"http://hemin.live/archives/1143",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("面向完全外行的chatGPT和大语言模型的介绍 – From nothing")]),_:1})]),e("p",null,[r(a,{href:"https://magazine.sebastianraschka.com/p/understanding-large-language-models?utm_source=direct&utm_campaign=post&utm_medium=web",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Understanding Large Language Models")]),_:1})]),e("p",null,[r(a,{href:"https://www.bilibili.com/video/BV1uu4y1m7ak",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("关于 AI 的深度研究：ChatGPT 正在产生心智吗？")]),_:1})]),e("p",null,[r(a,{href:"https://www.bilibili.com/video/BV1MY4y1R7EN/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("【渐构】万字科普GPT4为何会颠覆现有工作流；为何你要关注微软Copilot、文心一言等大模型 - 哔哩哔哩 bilibili")]),_:1})]),e("p",null,[r(a,{href:"https://www.bilibili.com/video/BV1VL411U7MU/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("2023年3月，人类终究走上了一条无法回头的路 - 哔哩哔哩 bilibili")]),_:1})]),e("p",null,[r(a,{href:"https://www.bilibili.com/video/BV1v3411r78R",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("强烈推荐！台大李宏毅自注意力机制和Transformer详解！- 哔哩哔哩 bilibili")]),_:1})]),be,e("ul",null,[e("li",null,[r(a,{href:"https://blog.wtf.sg/posts/2023-02-03-the-new-xor-problem/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("when trees fall... | The New XOR Problem")]),_:1})]),e("li",null,[r(a,{href:"https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("hackerllama - The Random Transformer")]),_:1})]),e("li",null,[r(a,{href:"https://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.")]),_:1})]),e("li",null,[r(a,{href:"https://levelup.gitconnected.com/understanding-transformers-from-start-to-end-a-step-by-step-math-example-16d4e64e6eb1",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Solving Transformer by Hand: A Step-by-Step Math Example | by Fareed Khan | Dec, 2023 | Level Up Coding")]),_:1})]),e("li",null,[r(a,{href:"https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e",target:"_blank",rel:"noreferrer"},{default:n(()=>[t("Normcore LLM Reads")]),_:1})])]),r(i),r(s)])}const ze=u(H,[["render",Pe]]);export{we as __pageData,ze as default};
